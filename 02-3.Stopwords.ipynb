{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "# **Stop Words**\n",
    "\n",
    "<br></br>\n",
    "## **1 불용어 처리**\n",
    "Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = 'I like such a Wonderful Snow Ice Cream'\n",
    "texts = texts.lower()\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(texts)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[::18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word   for word in tokens   \n",
    "                 if word not in stopwords.words('english')]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **2 한글의 불용어 처리**\n",
    "인터넷에 공개되어 있는 불용어100 자료 (idf 값까지 txt에는 포함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/stopword_kr.txt', 'r', encoding='utf-8')\n",
    "s = f.read(); f.close()\n",
    "\n",
    "stop_words = [ txt.split('\\t')[:3]  for txt in s.split('\\n') ]\n",
    "stopword   = {}\n",
    "for txt in stop_words:\n",
    "    try:    stopword[txt[0]] = txt[1]\n",
    "    except: pass\n",
    "stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## **3 지속성장 보고서 불용어 추출**\n",
    "보고서간의 특징구별 단어들 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './data/kr-Report_2018.txt'\n",
    "f        = open(filename, 'r', encoding='utf-8')\n",
    "texts    = f.read()\n",
    "f.close()\n",
    "texts[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter   = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "texts     = texts.replace('\\n', ' ')   # 해당줄의 줄바꿈 내용 제거\n",
    "tokenizer = re.compile(r'[^ ㄱ-힣]+')   # 한글과 띄어쓰기를 제외한 모든 글자를 선택\n",
    "tokens    = tokenizer.sub('', texts)   # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
    "tokens    = tokens.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_token = []\n",
    "for token in tokens:\n",
    "    token_pos = twitter.pos(token)\n",
    "    temp      = [txt_tag[0]   for txt_tag in token_pos  \n",
    "                              if txt_tag[1] == 'Noun']\n",
    "    if len(\"\".join(temp)) > 1:\n",
    "        noun_token.append(\"\".join(temp))\n",
    "\n",
    "texts = \" \".join(noun_token)\n",
    "texts[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "texts = word_tokenize(texts)\n",
    "texts[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords.txt : 2015, 2016, 2017, 2018년 모두 존재하는 단어목록\n",
    "f         = open('./data/stopwords.txt', 'r', encoding='utf-8')\n",
    "stopwords = f.read(); f.close()\n",
    "stopwords = stopwords.split(' ')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text for text in texts  \n",
    "              if text not in stopwords]\n",
    "\n",
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "freqtxt = pd.Series(dict(FreqDist(texts))).sort_values(ascending=False)\n",
    "freqtxt[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "twitter.pos('가치창출')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter.pos('갤러시')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "wcloud = WordCloud('./data/D2Coding.ttf',\n",
    "                   relative_scaling = 0.2,\n",
    "                   background_color = 'white').generate(\" \".join(texts))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(wcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
